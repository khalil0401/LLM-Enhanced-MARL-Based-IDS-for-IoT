{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# IoT-23 Dataset Preprocessing for Kaggle\n",
                "\n",
                "**Purpose:** Download and preprocess IoT-23 dataset into HDF5 format\n",
                "\n",
                "**Input:** Raw IoT-23 PCAP files (from internet or uploaded)\n",
                "\n",
                "**Output:** Preprocessed HDF5 file ready for MAPPO training\n",
                "\n",
                "**Time:** ~2-3 hours (CPU only, no GPU needed)\n",
                "\n",
                "---\n",
                "\n",
                "## Instructions:\n",
                "1. **Choose Session Type:** CPU (no GPU needed for preprocessing)\n",
                "2. **Enable Internet:** ON (to download IoT-23)\n",
                "3. **Run All Cells**\n",
                "4. **Download Output:** Save as Kaggle dataset for training notebook"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üì¶ Cell 1: Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "print(\"Installing preprocessing dependencies...\")\n",
                "!pip install -q scapy\n",
                "!pip install -q pandas\n",
                "!pip install -q h5py\n",
                "!pip install -q numpy\n",
                "!pip install -q scikit-learn\n",
                "!pip install -q tqdm\n",
                "\n",
                "print(\"‚úÖ Dependencies installed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üì• Cell 2: Download IoT-23 Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "import os\n",
                "import urllib.request\n",
                "from tqdm import tqdm\n",
                "\n",
                "# IoT-23 dataset URLs\n",
                "# Note: Using smaller captures for Kaggle (full dataset is ~25GB)\n",
                "IOT23_URLS = [\n",
                "    # Scenario 1: Mirai botnet (C&C + Attack)\n",
                "    \"https://mcfp.felk.cvut.cz/publicDatasets/IoT-23-Dataset/IndividualScenarios/CTU-IoT-Malware-Capture-1-1/2018-12-21-capture.pcap\",\n",
                "    # Scenario 9: Philips HUE IoT (benign + attack)\n",
                "    \"https://mcfp.felk.cvut.cz/publicDatasets/IoT-23-Dataset/IndividualScenarios/CTU-IoT-Malware-Capture-9-1/2019-01-09-capture.pcap\",\n",
                "]\n",
                "\n",
                "# Create download directory\n",
                "os.makedirs('/kaggle/working/raw_pcap', exist_ok=True)\n",
                "\n",
                "print(\"Downloading IoT-23 dataset...\")\n",
                "print(\"This may take 30-60 minutes depending on connection speed\\n\")\n",
                "\n",
                "for i, url in enumerate(IOT23_URLS, 1):\n",
                "    filename = f\"scenario_{i}.pcap\"\n",
                "    filepath = f\"/kaggle/working/raw_pcap/{filename}\"\n",
                "    \n",
                "    if os.path.exists(filepath):\n",
                "        print(f\"‚úÖ {filename} already downloaded\")\n",
                "        continue\n",
                "    \n",
                "    print(f\"Downloading {filename}...\")\n",
                "    try:\n",
                "        urllib.request.urlretrieve(url, filepath)\n",
                "        size_mb = os.path.getsize(filepath) / 1e6\n",
                "        print(f\"‚úÖ Downloaded {filename} ({size_mb:.1f} MB)\")\n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Error downloading {filename}: {e}\")\n",
                "        print(\"   Continuing with available files...\")\n",
                "\n",
                "# List downloaded files\n",
                "print(\"\\nDownloaded files:\")\n",
                "!ls -lh /kaggle/working/raw_pcap/"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîß Cell 3: Parse PCAP Files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "from scapy.all import rdpcap, IP, TCP, UDP\n",
                "import pandas as pd\n",
                "from tqdm import tqdm\n",
                "import numpy as np\n",
                "\n",
                "def parse_pcap(pcap_file, max_packets=100000):\n",
                "    \"\"\"\n",
                "    Parse PCAP file and extract features\n",
                "    \n",
                "    Args:\n",
                "        pcap_file: Path to PCAP file\n",
                "        max_packets: Maximum packets to process (for memory)\n",
                "    \"\"\"\n",
                "    print(f\"\\nParsing {pcap_file}...\")\n",
                "    \n",
                "    try:\n",
                "        packets = rdpcap(pcap_file)\n",
                "        print(f\"  Total packets: {len(packets)}\")\n",
                "        \n",
                "        # Limit for memory\n",
                "        if len(packets) > max_packets:\n",
                "            print(f\"  Sampling {max_packets} packets for memory efficiency\")\n",
                "            packets = packets[:max_packets]\n",
                "        \n",
                "        # Extract features\n",
                "        features = []\n",
                "        for pkt in tqdm(packets, desc=\"Extracting features\"):\n",
                "            if IP in pkt:\n",
                "                feature = {\n",
                "                    'timestamp': float(pkt.time),\n",
                "                    'src_ip': pkt[IP].src,\n",
                "                    'dst_ip': pkt[IP].dst,\n",
                "                    'protocol': pkt[IP].proto,\n",
                "                    'packet_size': len(pkt),\n",
                "                    'src_port': pkt[TCP].sport if TCP in pkt else (pkt[UDP].sport if UDP in pkt else 0),\n",
                "                    'dst_port': pkt[TCP].dport if TCP in pkt else (pkt[UDP].dport if UDP in pkt else 0),\n",
                "                    'flags': pkt[TCP].flags if TCP in pkt else 0,\n",
                "                }\n",
                "                features.append(feature)\n",
                "        \n",
                "        df = pd.DataFrame(features)\n",
                "        print(f\"  Extracted {len(df)} flow records\")\n",
                "        return df\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"  ‚ùå Error parsing {pcap_file}: {e}\")\n",
                "        return pd.DataFrame()\n",
                "\n",
                "# Parse all PCAP files\n",
                "all_dataframes = []\n",
                "\n",
                "pcap_files = !ls /kaggle/working/raw_pcap/*.pcap\n",
                "for pcap_file in pcap_files:\n",
                "    df = parse_pcap(pcap_file.strip())\n",
                "    if not df.empty:\n",
                "        all_dataframes.append(df)\n",
                "\n",
                "# Combine all dataframes\n",
                "if all_dataframes:\n",
                "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
                "    print(f\"\\n‚úÖ Total flow records: {len(combined_df)}\")\n",
                "else:\n",
                "    print(\"\\n‚ùå No data extracted\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üè∑Ô∏è Cell 4: Label Data (Simplified)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "# Simple heuristic labeling for IoT-23\n",
                "# In production, use actual labels from IoT-23 metadata\n",
                "\n",
                "def label_traffic(df):\n",
                "    \"\"\"\n",
                "    Simple heuristic labeling\n",
                "    Note: IoT-23 provides actual labels - this is simplified for demo\n",
                "    \"\"\"\n",
                "    df['label'] = 0  # Default: benign\n",
                "    \n",
                "    # Heuristic 1: High packet rate to single IP\n",
                "    ip_counts = df['dst_ip'].value_counts()\n",
                "    high_traffic_ips = ip_counts[ip_counts > 1000].index\n",
                "    df.loc[df['dst_ip'].isin(high_traffic_ips), 'label'] = 1\n",
                "    \n",
                "    # Heuristic 2: Unusual ports (known malware ports)\n",
                "    malware_ports = [23, 2323, 5555, 7547, 37215, 52869]\n",
                "    df.loc[df['dst_port'].isin(malware_ports), 'label'] = 1\n",
                "    \n",
                "    # Heuristic 3: Very large or very small packets\n",
                "    df.loc[(df['packet_size'] < 40) | (df['packet_size'] > 1500), 'label'] = 1\n",
                "    \n",
                "    attack_count = (df['label'] == 1).sum()\n",
                "    attack_ratio = attack_count / len(df)\n",
                "    \n",
                "    print(f\"Labeling results:\")\n",
                "    print(f\"  Benign: {(df['label'] == 0).sum()} ({100*(1-attack_ratio):.1f}%)\")\n",
                "    print(f\"  Attack: {attack_count} ({100*attack_ratio:.1f}%)\")\n",
                "    \n",
                "    return df\n",
                "\n",
                "combined_df = label_traffic(combined_df)\n",
                "print(\"\\n‚úÖ Data labeled\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üî¢ Cell 5: Feature Engineering"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import numpy as np\n",
                "\n",
                "def engineer_features(df):\n",
                "    \"\"\"\n",
                "    Create statistical features for ML\n",
                "    \"\"\"\n",
                "    print(\"Engineering features...\")\n",
                "    \n",
                "    # Time-based features\n",
                "    df = df.sort_values('timestamp')\n",
                "    df['inter_arrival_time'] = df['timestamp'].diff().fillna(0)\n",
                "    \n",
                "    # Flow-based features (5-tuple aggregation)\n",
                "    df['flow_id'] = (df['src_ip'] + '_' + df['dst_ip'] + '_' + \n",
                "                     df['protocol'].astype(str) + '_' + \n",
                "                     df['src_port'].astype(str) + '_' + \n",
                "                     df['dst_port'].astype(str))\n",
                "    \n",
                "    # Aggregate by flows\n",
                "    flow_features = df.groupby('flow_id').agg({\n",
                "        'packet_size': ['mean', 'std', 'min', 'max', 'sum'],\n",
                "        'inter_arrival_time': ['mean', 'std'],\n",
                "        'timestamp': ['count', 'min', 'max'],\n",
                "        'label': 'max'  # If any packet in flow is attack, flow is attack\n",
                "    }).reset_index()\n",
                "    \n",
                "    # Flatten column names\n",
                "    flow_features.columns = ['_'.join(col).strip('_') for col in flow_features.columns.values]\n",
                "    \n",
                "    # Calculate flow duration\n",
                "    flow_features['flow_duration'] = (flow_features['timestamp_max'] - \n",
                "                                       flow_features['timestamp_min'])\n",
                "    \n",
                "    # Calculate packets per second\n",
                "    flow_features['packets_per_sec'] = (flow_features['timestamp_count'] / \n",
                "                                         (flow_features['flow_duration'] + 1e-6))\n",
                "    \n",
                "    print(f\"  Created {len(flow_features)} flow records with {len(flow_features.columns)} features\")\n",
                "    \n",
                "    return flow_features\n",
                "\n",
                "flow_data = engineer_features(combined_df)\n",
                "\n",
                "# Show sample\n",
                "print(\"\\nSample features:\")\n",
                "print(flow_data.head())\n",
                "print(\"\\n‚úÖ Feature engineering complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üíæ Cell 6: Save to HDF5 Format"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "import h5py\n",
                "import numpy as np\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# Prepare data for HDF5\n",
                "print(\"Preparing data for HDF5...\")\n",
                "\n",
                "# Select numeric features only\n",
                "feature_cols = [col for col in flow_data.columns \n",
                "                if col not in ['flow_id', 'label_max'] \n",
                "                and flow_data[col].dtype in [np.float64, np.int64, np.float32, np.int32]]\n",
                "\n",
                "X = flow_data[feature_cols].values\n",
                "y = flow_data['label_max'].values\n",
                "\n",
                "# Normalize features\n",
                "scaler = StandardScaler()\n",
                "X_scaled = scaler.fit_transform(X)\n",
                "\n",
                "# Train/val/test split (60/20/20)\n",
                "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.4, random_state=42)\n",
                "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
                "\n",
                "print(f\"\\nDataset splits:\")\n",
                "print(f\"  Train: {len(X_train)} samples\")\n",
                "print(f\"  Val:   {len(X_val)} samples\")\n",
                "print(f\"  Test:  {len(X_test)} samples\")\n",
                "\n",
                "# Save to HDF5\n",
                "output_path = '/kaggle/working/iot23_processed.h5'\n",
                "\n",
                "with h5py.File(output_path, 'w') as f:\n",
                "    # Training data\n",
                "    f.create_dataset('train/features', data=X_train, compression='gzip')\n",
                "    f.create_dataset('train/labels', data=y_train, compression='gzip')\n",
                "    \n",
                "    # Validation data\n",
                "    f.create_dataset('val/features', data=X_val, compression='gzip')\n",
                "    f.create_dataset('val/labels', data=y_val, compression='gzip')\n",
                "    \n",
                "    # Test data\n",
                "    f.create_dataset('test/features', data=X_test, compression='gzip')\n",
                "    f.create_dataset('test/labels', data=y_test, compression='gzip')\n",
                "    \n",
                "    # Metadata\n",
                "    f.attrs['num_features'] = X.shape[1]\n",
                "    f.attrs['feature_names'] = ','.join(feature_cols)\n",
                "    f.attrs['attack_ratio'] = float(y.sum() / len(y))\n",
                "\n",
                "file_size_mb = os.path.getsize(output_path) / 1e6\n",
                "print(f\"\\n‚úÖ Saved to {output_path} ({file_size_mb:.1f} MB)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ‚úÖ Cell 7: Verify and Summarize"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify HDF5 file\n",
                "print(\"Verifying HDF5 file...\\n\")\n",
                "\n",
                "with h5py.File(output_path, 'r') as f:\n",
                "    print(\"Dataset structure:\")\n",
                "    def print_structure(name, obj):\n",
                "        if isinstance(obj, h5py.Dataset):\n",
                "            print(f\"  {name}: {obj.shape} ({obj.dtype})\")\n",
                "    f.visititems(print_structure)\n",
                "    \n",
                "    print(\"\\nMetadata:\")\n",
                "    for key, value in f.attrs.items():\n",
                "        print(f\"  {key}: {value}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"PREPROCESSING COMPLETE!\")\n",
                "print(\"=\"*60)\n",
                "print(f\"\\nüìÅ Output file: {output_path}\")\n",
                "print(f\"üìä Size: {file_size_mb:.1f} MB\")\n",
                "print(f\"\\nüì• Next steps:\")\n",
                "print(\"   1. Download this file from Output tab\")\n",
                "print(\"   2. Upload as Kaggle dataset: 'iot23-processed'\")\n",
                "print(\"   3. Use in training notebook!\")\n",
                "print(\"\\nüí∞ Cost: $0 (Kaggle free CPU)\")\n",
                "print(\"‚è±Ô∏è  Time: ~2-3 hours\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}