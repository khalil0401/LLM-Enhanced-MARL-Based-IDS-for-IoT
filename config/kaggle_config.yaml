# Kaggle-Optimized Training Configuration
# Reduced resources to fit in Kaggle's free GPU tier
# GPU: P100 (16GB) or T4 (16GB)
# RAM: 13-16GB
# Session: 12 hours max

env_config:
  num_agents: 10
  observation_dim: 5000
  max_episode_steps: 1000
  dataset_path: "/kaggle/input/iot23-processed/iot23_processed.h5"  # Kaggle dataset path
  self_play: false  # Set to true for adversarial training

training:
  lr: 3.0e-4
  gamma: 0.99
  lambda: 0.95
  clip_param: 0.2
  train_batch_size: 2048       # REDUCED from 4096 for Kaggle
  sgd_minibatch_size: 64        # REDUCED from 128 for Kaggle
  num_sgd_iter: 10
  num_workers: 4                # REDUCED from 8 for Kaggle
  num_gpus: 1                   # Kaggle provides 1 GPU
  framework: "torch"
  vf_clip_param: 10.0
  entropy_coeff: 0.01

experiment:
  total_iterations: 500         # Start with 500, can extend in next session
  checkpoint_freq: 10           # Save frequently (every 10 iterations)
  evaluation_interval: 10
  checkpoint_dir: "/kaggle/working/checkpoints"  # Kaggle working directory
  wandb_project: "llm-marl-ids-kaggle"
  wandb_entity: "your-entity"

reward_weights:
  detect: 1.0
  fp: -0.5
  latency: -0.2
  resource: -0.1

# Kaggle-specific settings
kaggle:
  save_to_output: true          # Copy checkpoints to /kaggle/working for download
  resume_from_checkpoint: null  # Set to path if resuming: "/kaggle/input/checkpoint/checkpoint_000500"
  max_session_hours: 11.5       # Stop before 12h limit with margin
